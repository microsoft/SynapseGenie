{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Genie_Deltalake_AutoUpkeep process",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 152,
      "outputs": [],
      "metadata": {},
      "source": [
        "# This script collects statistics on various delta lake tables and determines if they need to be vacuumed\r\n",
        "# It optimizes and Vacuum all the delta lake tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Configs for Delta lake Maintenance **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Configs for Delta lake Maintenance \r\n",
        "\r\n",
        "# Decides the recent  timeperiod to retain history , Default 1 days (24 * 7 = 168 hours)\r\n",
        "vacuumRetentionInHours = 168\r\n",
        "\r\n",
        "# Low version count indicates the table has not been through any updates and does not need Maintenance(optimize and Vacuum)\r\n",
        "minVersionCount = 3\r\n",
        "\r\n",
        "# New tables within a certain time period will be ignore from Maintenance(optimize and Vacuum)\r\n",
        "ignore_New_Tabledays = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Configs for Delta lake execution\r\n",
        "\r\n",
        "# By default threads is set equal to the number of executor cores. \"threads\" variable can be tuned to a particular number depending on requirements\r\n",
        "\r\n",
        "import os\r\n",
        "executorCores = (sc._jsc.sc().getExecutorMemoryStatus().keySet().size()-1)*os.cpu_count() \r\n",
        "\r\n",
        "min_Workers_Heavyprocess = executorCores / 2\r\n",
        "# atleast 2 cores per thread will be provided\r\n",
        "\r\n",
        "max_Workers_Lightprocess = executorCores\r\n",
        "# atleast 1 core per thread will be provided"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**List all Databases of Spark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "fullDatabaseList = spark.sql(\"Show Databases\").rdd.map(lambda r : r[0]).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Blacklist databases** that dont need to be optimized through this script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Blacklist databases that dont need to be optimized through this script\r\n",
        "# ignoreDatabases = ['onehrsi','crm','aurora']\r\n",
        "ignoreDatabases = []\r\n",
        "\r\n",
        "dbList = [i for i in fullDatabaseList if i not in ignoreDatabases]\r\n",
        "\r\n",
        "print(dbList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# temp restriction for debugging (optional)\r\n",
        "\r\n",
        "# dbList = [ 'genie', 'temp']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class TableDetail:\r\n",
        "  def __init__(self, tableName, location, isDelta):     \r\n",
        "        # Instance Variable    \r\n",
        "        self.tableName = tableName\r\n",
        "        self.location = location \r\n",
        "        self.isDelta = isDelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Key Method to run tasks in parallel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import concurrent.futures\r\n",
        "\r\n",
        "workerCount=4\r\n",
        "exceptionDetails = []\r\n",
        "\r\n",
        "def runParallelTasks(taskName, paramList, workerCount):   \r\n",
        "    resultDict = {}\r\n",
        "\r\n",
        "    with concurrent.futures.ThreadPoolExecutor(workerCount) as executor:\r\n",
        "        taskResults = {executor.submit(taskName, param) : (param) for param in paramList}\r\n",
        "        for f in concurrent.futures.as_completed(taskResults.keys()):\r\n",
        "            result = f.result()\r\n",
        "            if result:\r\n",
        "                resultDict[taskResults[f]] = result      \r\n",
        "    return resultDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def flattenDictionaryValues(resultDict):\r\n",
        "    flatTabList = []\r\n",
        "    for k in resultDict.keys():\r\n",
        "        for t2 in resultDict[k]:\r\n",
        "            flatTabList.append(t2)\r\n",
        "    return flatTabList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Lists all tables in the Hive metastore**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def getAllTableNames(databaseName):\r\n",
        "    #code to fetch tables and verify if delta\r\n",
        "    print(\"Initiate get tables list for database: \"+ databaseName)\r\n",
        "    newTables=[]\r\n",
        "    if databaseName == \"default\":\r\n",
        "        return newTables\r\n",
        "    \r\n",
        "    tablesDF = spark.sql(\"Show tables from \"+databaseName).collect()\r\n",
        "    for t in tablesDF:\r\n",
        "        if databaseName == \"default\":\r\n",
        "            tName = t[1]\r\n",
        "            newTables.append(tName)\r\n",
        "        elif t[2] == False : \r\n",
        "            tName = databaseName + \".\" + t[1] \r\n",
        "            newTables.append(tName)  \r\n",
        "        \r\n",
        "        print( \"TableFullName : \" + databaseName+ \".\" + t[1]  )    \r\n",
        "    return newTables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_workers = max_Workers_Lightprocess\r\n",
        "\r\n",
        "tResults = runParallelTasks(getAllTableNames, dbList, max_workers)\r\n",
        "\r\n",
        "allTables = flattenDictionaryValues(tResults)\r\n",
        "print(\"All Tables found : \"+str(len(allTables)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Blacklist tables** that dont need to be optimized through this script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Blacklist tables that dont need to be optimized through this script\r\n",
        "# ignoreTables = ['ihr.tabfilesvw']\r\n",
        "ignoreTables = []\r\n",
        "\r\n",
        "allApprovedTables = [i for i in allTables if i not in ignoreTables]\r\n",
        "\r\n",
        "# print(allApprovedTables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import sys\r\n",
        "\r\n",
        "deltaTablesDetails = []\r\n",
        "\r\n",
        "def getDeltaDetails(tableName):\r\n",
        "  query = \"Describe detail \"+ tableName\r\n",
        "  result = False\r\n",
        "  try:\r\n",
        "    tableDetail = spark.sql(query).collect()\r\n",
        "    if tableDetail[0].format == \"delta\" and tableDetail[0].id is not None :\r\n",
        "      result = True \r\n",
        "      deltaTablesDetails.append(TableDetail(tableName, tableDetail[0].location, 'True') )\r\n",
        "    else:\r\n",
        "      result = False\r\n",
        "  except Exception as e:\r\n",
        "    exceptionInfo = sys.exc_info()[0]\r\n",
        "    # Non delta tables throw exception. Uncomment below to log all errors\r\n",
        "\r\n",
        "    # print(\"Exception occurred :\" + tableName)\r\n",
        "    # print(sys.exc_info()[0])\r\n",
        "    # print(str(e))    \r\n",
        "    # exceptionDetails.append((tableName,\"TableDetailStage\", str(e)))\r\n",
        "    # print(\"Exception occurred :\" + tableName +\" : \" + str(e))\r\n",
        "    \r\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_workers=max_Workers_Lightprocess\r\n",
        "\r\n",
        "deltaTablesDetails = []\r\n",
        "tResults = runParallelTasks(getDeltaDetails, allApprovedTables, max_workers)\r\n",
        "\r\n",
        "\r\n",
        "print(\"All Delta Tables found : \"+str(len(tResults)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class TableHistoryDetail:\r\n",
        "  def __init__(self, tableName, version, timestamp):     \r\n",
        "        # Instance Variable    \r\n",
        "        self.tableName = tableName\r\n",
        "        self.version = version \r\n",
        "        self.timestamp = timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# deltaTablesDetails\r\n",
        "tableHistoryDetails = []\r\n",
        "\r\n",
        "def getTableHistory(deltaTableDetail):\r\n",
        "    tabName = deltaTableDetail.tableName\r\n",
        "    try:\r\n",
        "        tabHistory = spark.sql(\"Describe history \"+ tabName).collect()\r\n",
        "        for thd in tabHistory:\r\n",
        "            tableHistoryDetails.append(TableHistoryDetail(tabName, thd.version, thd.timestamp))\r\n",
        "    except Exception as e:    \r\n",
        "        exceptionInfo = sys.exc_info()[0]\r\n",
        "        exceptionDetails.append((tabName,\"TableHistoryVersionStage\", str(e)))\r\n",
        "        print(\"Exception occurred at gathering History :\" + tabName)\r\n",
        "    \r\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Get History and version of tables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_workers = max_Workers_Lightprocess\r\n",
        "tableHistoryDetails = []\r\n",
        "tResults = runParallelTasks(getTableHistory, deltaTablesDetails, max_workers)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "field = [StructField(\"tableName\", StringType(), True), StructField(\"version\",LongType(), True),StructField(\"timestamp\", TimestampType(), True)]\r\n",
        "schema = StructType(field)\r\n",
        "\r\n",
        "cols = ['tableName','version', 'timestamp'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "tableHistoryList = []\r\n",
        "\r\n",
        "for k in tableHistoryDetails:\r\n",
        "        tableHistoryList.append([k.tableName, k.version, k.timestamp])\r\n",
        "    \r\n",
        "thDF = spark.createDataFrame(tableHistoryList, cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "thDF.createOrReplaceTempView(\"TabHistoryVw\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "CREATE OR REPLACE TEMPORARY VIEW TabVersionVW AS (\r\n",
        "Select TableName, count(*) as VersionCount, min(Timestamp) as OldestVersionDate,  max(Timestamp) as MostRecentVersionDate  from TabHistoryVw\r\n",
        "group by tableName\r\n",
        "order by 2 desc, 3 ASC\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "drop table if exists TabVersion;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "Create table if not exists TabVersion\r\n",
        "Select * from TabVersionVW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class TableFiles:\r\n",
        "  def __init__(self, tableName, location, fileCount, totalSizeInBytes, totalSizeInGBs):     \r\n",
        "        # Instance Variable    \r\n",
        "        self.tableName = tableName\r\n",
        "        self.location = location \r\n",
        "        self.fileCount = fileCount\r\n",
        "        self.totalSizeInBytes = totalSizeInBytes\r\n",
        "        self.totalSizeInGBs = totalSizeInGBs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_dir_content(ls_path):\r\n",
        "  baseFilesInfo = mssparkutils.fs.ls(ls_path)\r\n",
        "  subdir_filesInfo = [get_dir_content(p.path) for p in baseFilesInfo if p.isDir() and p.path != ls_path]\r\n",
        "  flat_subdir_FilesInfo = [p for subFileInfo in subdir_filesInfo for p in subFileInfo]\r\n",
        "  return list(map(lambda p: p, baseFilesInfo)) + flat_subdir_FilesInfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "tabFiles = []\r\n",
        "def getTableFileDetails(tabDetail):\r\n",
        "    tabName = tabDetail.tableName\r\n",
        "    tabLocation = tabDetail.location\r\n",
        "    \r\n",
        "    try:\r\n",
        "        files =  get_dir_content(tabLocation)\r\n",
        "        fileCount = len(files)\r\n",
        "        s = 0\r\n",
        "        for f in files:\r\n",
        "            s = s + f.size\r\n",
        "        totalSizeInBytes = float(s)\r\n",
        "        totalSizeInGBs = float(s) / 1073741824\r\n",
        "        tabFile = TableFiles(tabName, tabLocation, fileCount, totalSizeInBytes, totalSizeInGBs)\r\n",
        "        print(tabName+\" : \"+ str(totalSizeInGBs) + \"GB\")\r\n",
        "        tabFiles.append(tabFile)\r\n",
        "    except Exception as e:    \r\n",
        "        exceptionInfo = sys.exc_info()[0]\r\n",
        "        exceptionDetails.append((tabName,\"ListFilesInfoStage\", str(e)))\r\n",
        "        print(\"Exception occurred at gathering files info stage :\" + tabName)\r\n",
        "\r\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_workers = max_Workers_Lightprocess\r\n",
        "tabFiles = []\r\n",
        "\r\n",
        "tResults = runParallelTasks(getTableFileDetails, deltaTablesDetails, max_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "tabFilesDF = spark.createDataFrame(tabFiles, None)\r\n",
        "tabFilesDF.createOrReplaceTempView(\"tabFilesVW\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "CREATE OR REPLACE TEMPORARY VIEW DeltaLakeTablesVW AS \r\n",
        "(\r\n",
        "Select TV.TableName, totalSizeInGBs, OldestversionDate, VersionCount, FileCount, location, totalSizeInBytes, MostRecentVersionDate\r\n",
        "-- from TabVersionVW TV\r\n",
        "from TabVersion TV\r\n",
        "left Join tabFilesVW TF on TF.tableName = TV.tableName \r\n",
        "order by totalSizeInGBs desc, VersionCount desc\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "Drop table if exists DeltaLakeTables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "Create table if not exists DeltaLakeTables\r\n",
        "Select * from DeltaLakeTablesVW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Display data statistics of Delta lake tables \r\n",
        "Select * from DeltaLakeTables\r\n",
        "order by totalSizeInGBs desc, VersionCount desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Delta Lake Size for this ADB workspace\r\n",
        "select sum(totalSizeInGBs) from DeltaLakeTables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Delta Lake Size for this ADB workspace that can be cleaned up with Vacuum scripts\r\n",
        "Select sum(totalSizeInGBs) from DeltaLakeTables where VersionCount >= 7  and DateDiff(Current_TimeStamp(),OldestVersionDate) > 7 --  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "scriptGenerateQuery = f\"\"\"Select distinct Concat('Optimize ', tableName) as optimizeScript, Concat('Vacuum ', tableName, ' retain {vacuumRetentionInHours} hours') As vacuumScript \r\n",
        "from DeltaLakeTables \r\n",
        "where VersionCount >= {minVersionCount}  and DateDiff(Current_TimeStamp(),OldestVersionDate) > {ignore_New_Tabledays} \"\"\"\r\n",
        "\r\n",
        "print(scriptGenerateQuery) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "runScripts = []\r\n",
        "\r\n",
        "runScripts = spark.sql(scriptGenerateQuery). select(\"optimizeScript\",\"vacuumScript\").rdd.map(lambda r : (r[0],r[1])).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def execOptimizeVacuum(query):\r\n",
        "    print(\"Initiate optimize :  \"+ query[0])\r\n",
        "    spark.sql(query[0])\r\n",
        "\r\n",
        "    print(\"completed optimize :  \"+ query[0])\r\n",
        "\r\n",
        "    spark.sql(query[1])\r\n",
        "    print(\"completed vacuum :  \"+ query[1])\r\n",
        "    \r\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_workers = min_Workers_Heavyprocess\r\n",
        "\r\n",
        "tResults = runParallelTasks(execOptimizeVacuum, runScripts, max_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Clean up with below script once done\r\n",
        "drop table if exists TabVersion;\r\n",
        "drop table if exists DeltaLakeTables;"
      ]
    }
  ]
}
